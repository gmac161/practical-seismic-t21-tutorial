{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47657c4a",
   "metadata": {},
   "source": [
    "![swung logo](images/swung_logo_vector.png)\n",
    "# Transform 21: Practical Seismic in Python #\n",
    "# Graeme Mackenzie & JÃ¸rgen Kvalsvik  #\n",
    "\n",
    "## Introduction\n",
    "\n",
    "During this tutorial we are going to demonstrate how to read in a 4D base and monitor volume using 2 different python packages, plot some lines and slices interactively then calculate some simple 4D attributes (4D difference and NRMS) and apply a frequency filter to the data.\n",
    "\n",
    "### Video Recording\n",
    "The corresponding livestream and resultant video can be found [here](https://www.youtube.com/watch?v=EH7u74Vyays) on YouTube. Note that the livestream is about 20-30 seconds behind us!\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### Python Packages\n",
    "\n",
    "In addition to the standard packages we will need\n",
    " - segyio\n",
    " - seismic-zfp\n",
    " - scipy\n",
    " - holoviz\n",
    " \n",
    "If you are using Anaconda these can be installed using:\n",
    "\n",
    "    conda install segyio scipy \n",
    "    conda install -c pyviz holoviz\n",
    "    pip install seismic-zfp\n",
    " \n",
    "Or use the environment.yml file in the GitHub repository. \n",
    "\n",
    "If you are using python for windows these will all need to be installed through pip install. However it is much easier to install holoviz using Anaconda (if you are using pip you will likely need to download and install the individual whl files)\n",
    "\n",
    "#### Data\n",
    "\n",
    "We are going to use 2 volumes from the Volve data which has been published by Equinor.  You can read about it [here](https://www.equinor.com/en/news/14jun2018-disclosing-volve-data.html) and download the data [here](https://data.equinor.com/dataset/Volve)\n",
    "\n",
    "We will use data from the 4D processing of the ST0202 and ST10010 surveys which are in the ST0202vsST10010_4D sub directory\n",
    "\n",
    "- ST0202ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534.segy\n",
    "- ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534.segy\n",
    "\n",
    "\n",
    "Instructions for downloading the data using Azure Storage Explorer are provided on the download link but we can also download the SEG-Ys using the azure cli and the code in the cells below\n",
    "\n",
    "You will need to get a shared access token from the [Volve data webpage](https://data.equinor.com/dataset/Volve) as illustrated in the image below.\n",
    "\n",
    "![sas token](images/access_token.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download the SEG-Ys from volve we use the azure cli\n",
    "import sys\n",
    "!{sys.executable} -m pip install azure-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# The shared access signature URI can be found on the Volve download page. The token is the stuff after ?, should be sv=<date>&sr=<sr>...\n",
    "sastoken = '<sas-token-here>'\n",
    "if not os.path.exists('ST10010.segy'):\n",
    "    !{sys.executable} -m azure.cli storage blob download \\\n",
    "        --account-name datavillagesa \\\n",
    "        --container-name volve \\\n",
    "        --name \"Seismic/ST0202vsST10010_4D/Stacks/ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534.segy\" \\\n",
    "        --file \"ST10010.segy\" \\\n",
    "        --sas-token \"{sastoken}\"\n",
    "\n",
    "if not os.path.exists('ST0202.segy'):\n",
    "    !{sys.executable} -m azure.cli storage blob download \\\n",
    "        --account-name datavillagesa \\\n",
    "        --container-name volve \\\n",
    "        --name \"Seismic/ST0202vsST10010_4D/Stacks/ST0202ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534.segy\" \\\n",
    "        --file \"ST0202.segy\" \\\n",
    "        --sas-token \"{sastoken}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd06690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Run this to have the notebook clean up after itself. It is not run automatically.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.remove('ST0202.segy')\n",
    "    os.remove('ST10010.segy')\n",
    "    os.remove('ST10010.sgz')   # We'll create this file during the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d0fe5",
   "metadata": {},
   "source": [
    "## Basic Imports\n",
    "\n",
    "We'll start with importing some packages; <br>\n",
    "- [NumPy](https://numpy.org/) (which we will use to hold the data once we have read it in)\n",
    "- [segyio](https://segyio.readthedocs.io/en/latest/) (to read the data)\n",
    "- [Matlotlib](https://matplotlib.org/) for visualizing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad95f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import segyio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e74b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default plot size for matplotlib figures\n",
    "matplotlib.rcParams['figure.figsize'] = (11.75, 8.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cacb4",
   "metadata": {},
   "source": [
    "## Reading the Volve 2002 base survey using segyio\n",
    "\n",
    "We will read the base 2002 survey using segyio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b42a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_segy = 'ST0202.segy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2b377",
   "metadata": {},
   "source": [
    "Let's check the EBCDIC to see that we have the data we are expecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728fc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = segyio.open(base_segy, ignore_geometry = True)\n",
    "segyio.tools.wrap(f.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c3758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(segyio.tools.wrap(f.text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f30a7d",
   "metadata": {},
   "source": [
    "The EBCDIC shows the first live sample should be at 4ms - let's check segyio is picking that up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8fae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.samples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6cffa",
   "metadata": {},
   "source": [
    "segyio is picking up the samples correctly and from the EBCDIC it looks like it is a regular volume, so let's try to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90299ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with segyio.open(base_segy) as segyf:\n",
    "    n_traces = segyf.tracecount\n",
    "    sample_rate = segyio.tools.dt(segyf)\n",
    "    n_samples = segyf.samples.size\n",
    "    n_il = len(segyf.iline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa0e1f",
   "metadata": {},
   "source": [
    "So it seems the data is not as regular as we thought.\n",
    "Let's look into the data in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = segyio.open(base_segy, ignore_geometry = True)\n",
    "ntraces    = len(f.trace)\n",
    "inlines    = []\n",
    "crosslines = []\n",
    "\n",
    "for i in range(ntraces):\n",
    "    headeri = f.header[i]\n",
    "    inlines.append(headeri[segyio.su.iline])\n",
    "    crosslines.append(headeri[segyio.su.xline])\n",
    "\n",
    "print(f'{ntraces} traces')\n",
    "print(f'first 10 inlines: {inlines[:10]}')\n",
    "print(f'first 10 crosslines: {crosslines[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8830c3",
   "metadata": {},
   "source": [
    "The loop variable *i* is only used to look up the the right header, but segyio can manage this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inlines = []\n",
    "crosslines = []\n",
    "for h in f.header:\n",
    "    inlines.append(h[segyio.su.iline])\n",
    "    crosslines.append(h[segyio.su.xline])\n",
    "        \n",
    "print(f'{ntraces} traces')\n",
    "print(f'first 10 inlines: {inlines[:10]}')\n",
    "print(f'first 10 crosslines: {crosslines[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfad4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the inline and crossline as a scatter plot\n",
    "plt.scatter(crosslines, inlines, marker=\"s\", s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af890d6",
   "metadata": {},
   "source": [
    "We can tell from the top-left and bottom-right corner that there are a few traces missing, and the file is not a perfect cube.\n",
    "\n",
    "Using a scatter plot works because the file is _small enough_ - with very large files this becomes impractically slow to run, and likely impossible to see, but it's a good trick for a lot of files.\n",
    "\n",
    "We can approach it more programmatically to understand precisely what is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88392163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "uniqil = set(inlines)\n",
    "uniqxl = set(crosslines)\n",
    "real = set(zip(inlines, crosslines))\n",
    "grid = set(itertools.product(uniqil, uniqxl))\n",
    "missing = grid - real\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb288e5e",
   "metadata": {},
   "source": [
    "To continue, we need to check if the line numbers are regularly spaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(crosslines[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231ec68",
   "metadata": {},
   "source": [
    "Plotting is good for eyeballing and catching some _obviously_ disorganised files, but it does not give too much confidence, because small errors are difficult to spot.\n",
    "\n",
    "We want to fill out the missing traces and create a regular volume. We start by determining the interval of inlines and crosslines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ils = np.unique(inlines)\n",
    "xls = np.unique(crosslines)\n",
    "inline_interval    = ils[1:] - ils[:-1]\n",
    "crossline_interval = xls[1:] - xls[:-1]\n",
    "print(inline_interval)\n",
    "print(crossline_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86268955",
   "metadata": {},
   "source": [
    "This file is obviously all regular and with a neat delta 1. Here you might need to make some choices should the interval be very irregular, but for this file we are good.\n",
    "\n",
    "We now need to put every trace in the right place, which means we need to map in/crossline pairs to the right offset in our target array. Had no traces been missing, this is what segyio figures out by default.\n",
    "\n",
    "Note: There are faster approaches, but this is fairly straight forward to read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ils = sorted(uniqil)\n",
    "xls = sorted(uniqxl)\n",
    "lineindex = {\n",
    "    (il, xl): i\n",
    "    for i, (il, xl) in enumerate(sorted(grid))\n",
    "\n",
    "}\n",
    "lineindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ee1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros((len(ils), len(xls), len(f.samples)))\n",
    "lineard = d.reshape(d.shape[0] * d.shape[1], d.shape[2])\n",
    "for il, xl, trace in zip(inlines, crosslines, f.trace[:]):\n",
    "    lineard[lineindex[il, xl]][:] = trace[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61429d2",
   "metadata": {},
   "source": [
    "Data is in a 3D numpy array and our inline, crossline and twt are also in separate numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d64148",
   "metadata": {},
   "source": [
    "## Plotting the data\n",
    "As geophysicists one of the first things we want to do once we've loaded some data is to view it. We're going to use Matplotlib to vizualize a single (central) inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fbd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some aliases\n",
    "ilines = np.array(sorted(uniqil))\n",
    "xlines = np.array(sorted(uniqxl))\n",
    "t = np.array(f.samples)\n",
    "\n",
    "# Estimate the amplitude range to use for the plots by taking the 95th percentile\n",
    "\n",
    "vm = np.percentile(d, 95)\n",
    "print(vm)\n",
    "\n",
    "# Define the central line\n",
    "central = len(ilines) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "plt.imshow(d[central,:,:], cmap='gray', aspect='auto', vmin=-vm, vmax=vm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578655b",
   "metadata": {},
   "source": [
    "We can see that it's definitely seismic but not oriented how we would expect.<br>  \n",
    "When using any plotting packages in Python we need to consider the origin and how the data is read.  Matplotlib assumes the origin is in the top right corner (which is fine for a section but possibly not what we would want if plotting a time slice) and it is plotting the trace along the x-axis. We need to transpose the array so it plots with the time on the y-axis <br>\n",
    "The axis labels aren't really meaningful to us so we also need to read the arrays containing the travel time and xline numbers and add these to the plot as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = [xlines[0], xlines[-1], t[-1], t[0]]\n",
    "\n",
    "# Plot the central inline in the usual orientation by transposing the data\n",
    "\n",
    "plt.imshow(d[central,:,:].T, cmap='gray', aspect='auto', vmin=-vm, vmax=vm, extent=extent)\n",
    "\n",
    "plt.xlabel('Xline')\n",
    "plt.ylabel('TWT (ms)')\n",
    "plt.title('ST0202 INL'+ str(ilines[central]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7faf43e",
   "metadata": {},
   "source": [
    "Note that whilst Matplotlib has the origin for the plot at the top right, the extent is coded as (left, right, bottom, top)\n",
    "(see [here](https://matplotlib.org/stable/tutorials/intermediate/imshow_extent.html) for more details on origins and extents in matplotlib.)\n",
    "\n",
    "We can also plot a slice, in this case we don't need transpose the data, but we have changed the origin to the bottom left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aff087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central time slice\n",
    "\n",
    "tmid = len(t)//2\n",
    "\n",
    "extent = [xlines[0], xlines[-1], ilines[0], ilines[-1]]\n",
    "\n",
    "plt.imshow(d[:,:,tmid], cmap='gray', origin='lower', aspect='auto', vmin=-vm, vmax=vm, extent=extent)\n",
    "\n",
    "plt.xlabel('CRL')\n",
    "plt.ylabel('INL')\n",
    "plt.title('ST0202 Time Slice '+ str(t[tmid])+'ms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4411fa3",
   "metadata": {},
   "source": [
    "### Interactive plotting\n",
    "\n",
    "Ideally we would like the plots to be a bit more interactive, so we could zoom in on an area of interest or quickly scan through some lines, rather than having to edit the code above and rerun the cell above multiple times. <br>\n",
    "\n",
    "We're going to use the advance plotting options of hvplot and panel to create an interactive plot. The libraries we are going to use are all grouped and maintained together under the [HoloViz ecosystem](https://holoviz.org/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c42723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import panel as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default image size\n",
    "from holoviews import opts\n",
    "\n",
    "opts.defaults(\n",
    "    opts.Image(\n",
    "        width=800, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac5df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a plotting function, the input will be the inline\n",
    "\n",
    "def plot_inl(inl):\n",
    "    \"\"\"\n",
    "    Plot a single inline using hvplot\n",
    "    \"\"\"\n",
    "    idx = inl - ilines[0]\n",
    "    da = xr.DataArray(d[idx,:,:].T)    \n",
    "    p = da.hvplot.image(clim=(-vm, vm), cmap='gray', flip_yaxis=True) \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inl(ilines[central])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use panel to pass an array of ilines to the function\n",
    "\n",
    "pn.interact(plot_inl, inl = ilines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c84e9e",
   "metadata": {},
   "source": [
    "The [widget](https://panel.holoviz.org/user_guide/Widgets.html) class in panels provides a range of widgets for the data selection (sliders, player, checkboxes, auto-complete text-field, .....) <br>\n",
    "Here we will use a drop down menu which is a little more useful for line selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34eac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_select = pn.widgets.Select(name='INL Selection', options=ilines.tolist())\n",
    "\n",
    "pn.interact(plot_inl, inl = line_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b5133",
   "metadata": {},
   "source": [
    "## Using Xarray\n",
    "\n",
    "As with our original plot using Matplotlib the interactive plots we created above have no useful axis info. <br>\n",
    "In the *plot_inl* function we had to convert our NumPy array into an DataArray in order to allow us to plot it with hvplot.  However, [Xarray](http://xarray.pydata.org/en/stable/) has a lot more useful functionality that makes it ideal for using with seismic data.  Xarray simplifies working with multi-dimensional data and allows dimension, coordinate and attribute labels to be added to the data (segysak utilises it and [Tony's tutorial on Tuesday](https://www.youtube.com/watch?v=hjzTH14va4o) provided some more information on the format).<br>\n",
    "Xarray has two data structures\n",
    "- DataArray: for a single data variable\n",
    "- DataSet: a container for multiple Data Arrays that share the same coordinates\n",
    "\n",
    "The figure below [(Hoyer & Hamman, 2017)](https://openresearchsoftware.metajnl.com/articles/10.5334/jors.148/) illustrates the concept of a dataset containing climate data\n",
    "![xarray example](images/xarray.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Data Array\n",
    "da = xr.DataArray(data = d,\n",
    "                  dims = ['il','xl','twt'],\n",
    "                  coords = {'il': ilines,\n",
    "                          'xl': xlines,\n",
    "                          'twt': t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec2fa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f44658",
   "metadata": {},
   "source": [
    "We now have our inline, crossline and travel time held as coordinates within inside the same array as the data itself. <br>  This allows us to select data either using the standard NumPy numerical indexing and slicing or using xarray's `.sel` label based indexing, allowing us to select data based on the coordinate. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cdd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.sel(twt = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "da[:,:,749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select a range we use slice\n",
    "da.sel(twt = slice(2000,3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f630a1c",
   "metadata": {},
   "source": [
    "We've converted the 2002 seismic into a DataArray, but as we're going to be reading another vintage, it's useful to convert it into a DataSet which we can add the 2010 monitor survey into. <br> \n",
    "We'll also add some attribute information both to the data array and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some attribute information to the datarray\n",
    "da.attrs['Year'] = '2002'\n",
    "da.attrs['Type'] = 'Final PSDM time converted'\n",
    "\n",
    "# Create a dataset\n",
    "volve_ds = da.to_dataset(name = 'base')\n",
    "\n",
    "# Add some attribute information to the dataset\n",
    "volve_ds.attrs['Country'] = 'Norway'\n",
    "volve_ds.attrs['Field'] = 'Volve'\n",
    "\n",
    "# Take a look\n",
    "volve_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34cd5a",
   "metadata": {},
   "source": [
    "Getting data from the Xarray dataset is similar to a python dictionary key and value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a line from the base survey\n",
    "volve_ds['base'].sel(il = 9963)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2df74",
   "metadata": {},
   "source": [
    "## Reading the Volve 2010 monitor survey using seismic-zfp\n",
    "\n",
    "Now we've read and reviewed one vintage, let's read the second\n",
    "\n",
    "Another option for reading SEG-Y into Python is to use [SEGY-SAK](https://segysak.readthedocs.io/en/latest/) which Tony Hallam gave a [tutorial](https://www.youtube.com/watch?v=hjzTH14va4o) on earlier this week, and which leverages segyio.  Here we'll use a third option and convert the segy file into another format.  We will use [seismic-zfp](https://github.com/equinor/seismic-zfp) which converts the SEG-Y to a compressed format [Wade, 2020](https://www.earthdoc.org/content/papers/10.3997/2214-4609.202032080).  We won't go into detail on this format but examples can be found on Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e56339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismic_zfp.conversion import SegyConverter, SgzReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6912b9",
   "metadata": {},
   "source": [
    "### Conversion from segy to seismic-zfp format\n",
    "\n",
    "The first step is to convert the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_segy = 'ST10010.segy'\n",
    "monitor_sgz = 'ST10010.sgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"standard\" SGZ file with 8:1 compression, using in-memory method\n",
    "\n",
    "with SegyConverter(monitor_segy) as converter:\n",
    "    converter.run(monitor_sgz, bits_per_voxel=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd915540",
   "metadata": {},
   "source": [
    "### Read seismic-zfp\n",
    "Now it's converted we can read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cube from zgy\n",
    "with SgzReader(monitor_sgz) as reader:\n",
    "    n_traces_m = reader.tracecount\n",
    "    n_il_m = reader.n_ilines\n",
    "    n_xl_m = reader.n_xlines\n",
    "    num_samples_m = reader.n_samples\n",
    "    ilines_m = reader.ilines\n",
    "    xlines_m = reader.xlines\n",
    "    data_m = reader.read_volume()\n",
    "print(f'sgz file with {n_traces_m} traces ({n_il_m} inlines, {n_xl_m} xlines) read')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a3b58",
   "metadata": {},
   "source": [
    "With seismic-zfp we have the initial overhead of converting the file, but then reading the sgz file may take less time than segyio.  Whether you use segyio, seismic-zfp or segysak comes down a lot of things; how often will you read the file, are the headers critical, is the compression important, personal preference,..... <br>\n",
    "\n",
    "Now we've read the monitor survey, let's quickly check they are both the same shape and have the same inline, crossline range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of arrays\n",
    "print(f'2002 base survey has shape {d.shape}')\n",
    "print(f'2010 monitor has shape {data_m.shape}')\n",
    "# Line ranges\n",
    "print(f'Base survey; inline: {ilines.min()} - {ilines.max()}, crossline: {xlines.min()} - {xlines.max()}')\n",
    "print(f'Monitor survey; inline: {ilines_m.min()} - {ilines_m.max()}, crossline: {xlines_m.min()} - {xlines_m.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ab577",
   "metadata": {},
   "source": [
    "Plot a slice of both volumes as a quick qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc966f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base    =      d[:, :, 75]\n",
    "monitor = data_m[:, :, 75]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16,6))\n",
    "\n",
    "extent = [xlines[0], xlines[-1], ilines[0], ilines[-1]]\n",
    "plt_kwargs = {'cmap':'gray', 'origin':'lower', 'aspect':'auto', 'vmin':-vm, 'vmax':vm, 'extent':extent}\n",
    "\n",
    "# The plt_kwargs replaces the various plotting options (suc as colour map etc) that are going to be common to both plots\n",
    "# Without it we would use a command similar to this\n",
    "#axs[0].imshow(base, cmap='gray', origin='lower', aspect='auto', vmin=-vm, vmax=vm, extent=extent)\n",
    "\n",
    "axs[0].imshow(base, **plt_kwargs)\n",
    "axs[0].set_title('2002 Base, Time Slice')\n",
    "axs[1].imshow(monitor, **plt_kwargs)\n",
    "axs[1].set_title('2010 Monitor, Time Slice')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e571a",
   "metadata": {},
   "source": [
    "Now we've quickly QC'd the monitor survey and satisfied ourselves that it is the same size and shape as and looks very similar to the base let's add it to the Xarray DataSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2 = xr.DataArray(data_m)\n",
    "da2.attrs['Year'] = '2010'\n",
    "volve_ds['monitor'] = (['il','xl','twt'],da2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c524b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "volve_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff429b1",
   "metadata": {},
   "source": [
    "# 5 MINUTE BREAK\n",
    "![break](images/break.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5fe26",
   "metadata": {},
   "source": [
    "## 4D Analysis\n",
    "Now we have the 2 volumes we can calculate some simple 4D attributes\n",
    "- Calculate the 4D difference \n",
    "- Calculate the NRMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d2b87",
   "metadata": {},
   "source": [
    "### 4D Difference\n",
    "\n",
    "The 4D difference is simply the monitor survey minus the base survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference of two arrays diff = volve_ds['monitor'] - volve_ds['base']\n",
    "# This will create a new array diff\n",
    "#\n",
    "# When we read in the monitor survey we firstly read it into a numpy array then added it to the xarray. \n",
    "# Here we will add the difference directly to the volve_ds dataset\n",
    "\n",
    "volve_ds['difference'] = (['il','xl','twt'], volve_ds['monitor'] - volve_ds['base'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7304331",
   "metadata": {},
   "source": [
    "Now we can plot the difference; here we'll create a plot with a drop down selection to either view the base, monitor or difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary for our data selection panel \n",
    "\n",
    "seismic = {}\n",
    "\n",
    "seismic['Base']          = volve_ds['base']\n",
    "seismic['Monitor']       = volve_ds['monitor']\n",
    "seismic['4D Difference'] = volve_ds['difference']\n",
    "\n",
    "# Define the plotting functions\n",
    "\n",
    "def plot_line_compare(inl, volume):\n",
    "    da = seismic[volume].sel(il=inl)   \n",
    "    p = da.hvplot.image(clim=(-vm,vm), cmap='gray', flip_yaxis=True, invert=True) \n",
    "    return p\n",
    "\n",
    "def plot_slice_compare(t, volume):\n",
    "    da = seismic[volume].sel(twt=t)    \n",
    "    p = da.hvplot.image(clim=(-vm,vm), cmap='gray') \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d28170",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_sel = pn.widgets.Select(name='INL Selection', options=volve_ds.coords['il'].values.tolist())\n",
    "\n",
    "pn.interact(plot_line_compare, inl=line_sel, volume=seismic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3507b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_sel = pn.widgets.Select(name='TWT', options=volve_ds.coords['twt'].values.tolist())\n",
    "\n",
    "t_sel = volve_ds.coords['twt'].values.astype(int)\n",
    "\n",
    "pn.interact(plot_slice_compare, t=t_sel, volume=seismic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c617d7",
   "metadata": {},
   "source": [
    "### NRMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e079db4",
   "metadata": {},
   "source": [
    "It's pretty difficult to see where the genuine 4D signal is. A standard 4D QC is NRMS (Kragh and Christie, 2002) which is a measure of similarity between two traces (from 0 - 200%). \n",
    "\n",
    "$$\n",
    "NRMS = \\frac{200 x RMS(a_t - b_t)}{RMS(a_t) + RMS (b_t)}\n",
    "$$\n",
    "\n",
    "where RMS is :\n",
    "$$\n",
    "RMS(x_t) = \\sqrt{\\frac{\\sum_{t1}^{t2}{(x_t)^2}}{N}}\n",
    "$$\n",
    "\n",
    "Ideally the NRMS value should be as low as possible above the reservoir where there should be no difference.\n",
    "\n",
    "Kragh, E. and Christie, P. (2002) Seismic repeatability, normalized RMS, and predictability. The Leading Edge, 21, 640-647, http://dx.doi.org/10.1190/1.1497316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ed864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions we need to calculate NRMS\n",
    "# NRMS = 200 x [rms (b-m) / rms(b) + rms(m)]\n",
    "\n",
    "def rms (data):\n",
    "    \"\"\"\n",
    "    Calculate the RMS \n",
    "    \"\"\"\n",
    "    rms = np.sqrt(np.mean(data**2, axis=2))\n",
    "    return rms\n",
    "\n",
    "def nrms (monitor, base):\n",
    "    \"\"\"\n",
    "    Calculate the NRMS between 2 surveys\n",
    "    \"\"\"\n",
    "    rmsB = rms(base)\n",
    "    rmsM = rms(monitor)\n",
    "    diff = monitor - base\n",
    "    rms_diff = rms(diff)\n",
    "    nrms = 200 * (rms_diff / (rmsB + rmsM))\n",
    "    return nrms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1100f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the NRMS and add to the xarray\n",
    "\n",
    "volve_ds['nrms_1_2'] = (['il','xl'], nrms(volve_ds['monitor'].sel(twt=slice(1000,2000)), volve_ds['base'].sel(twt=slice(1000,2000))))\n",
    "volve_ds['nrms_2_3'] = (['il','xl'], nrms(volve_ds['monitor'].sel(twt=slice(2000,3000)), volve_ds['base'].sel(twt=slice(2000,3000))))\n",
    "\n",
    "# Review the dataset\n",
    "\n",
    "volve_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce38a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some statistics\n",
    "\n",
    "mean1 = volve_ds['nrms_1_2'].mean().values\n",
    "mean2 = volve_ds['nrms_2_3'].mean().values\n",
    "\n",
    "print(f'Mean NRMS 1000-2000ms: {mean1}')\n",
    "print(f'Mean NRMS 2000-3000ms: {mean2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4935d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the NRMS as histograms\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (11,6), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten the arrays\n",
    "x1 = volve_ds['nrms_1_2'].values.flatten()\n",
    "x2 = volve_ds['nrms_2_3'].values.flatten()\n",
    "\n",
    "# Bin the NRMS values in a range 0,100 with 100 bins and plot the histograms\n",
    "\n",
    "im1 = axs[0].hist(x1, 100, range=[0,100])\n",
    "im2 = axs[1].hist(x2, 100, range=[0,100])\n",
    "\n",
    "# Add some statistics (utilising panda.describe())\n",
    "\n",
    "anchored_text0 = AnchoredText(str(volve_ds['nrms_1_2'].to_dataframe().describe()), loc=1)\n",
    "axs[0].add_artist(anchored_text0)\n",
    "\n",
    "anchored_text1 = AnchoredText(str(volve_ds['nrms_2_3'].to_dataframe().describe()), loc=1)\n",
    "axs[1].add_artist(anchored_text1)\n",
    "\n",
    "# Add some titles\n",
    "axs[0].set_title('NRMS 1000-2000ms')\n",
    "axs[1].set_title('NRMS 2000-3000ms')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the NRMS maps\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (16,8))\n",
    "\n",
    "# Define the map extent and common plotting parameters\n",
    "extent = [xlines[0], xlines[-1], ilines[0], ilines[-1]]\n",
    "plt_kwargs = {'cmap':'viridis_r', 'origin':'lower', 'aspect':'auto', 'extent':extent}\n",
    "\n",
    "# Plot the shallow window\n",
    "im1=axs[0].imshow(volve_ds['nrms_1_2'], **plt_kwargs, vmin=5, vmax=35)\n",
    "\n",
    "axs[0].set_title('NRMS 1000-2000ms')\n",
    "fig.colorbar(im1, ax=axs[0], orientation='horizontal', pad=0.06)\n",
    "\n",
    "# Plot the deeper window\n",
    "im2=axs[1].imshow(volve_ds['nrms_2_3'], **plt_kwargs, vmin=5, vmax=18)\n",
    "\n",
    "axs[1].set_title('NRMS 2000-3000ms')\n",
    "fig.colorbar(im2, ax=axs[1], orientation='horizontal', pad=0.06)\n",
    "\n",
    "# Add a title to the figure\n",
    "fig.suptitle('NRMS', size='xx-large', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec315bec",
   "metadata": {},
   "source": [
    "## Frequency spectra and filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e2914",
   "metadata": {},
   "source": [
    "### Frequency spectra\n",
    "\n",
    "The 4D QC's showed the data has a lot of 4D noise.  Let's take a look at the frequency spectra. <br>\n",
    "Both the NumPy and SciPy packages have the ability to run an fft, here we are using NumPy but we could equally  have used SciPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c16fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic function to calculate frequency spectra\n",
    "# As we are using an fft we should really taper or window the data to prevent edge effects, \n",
    "# but for this demonstration we are going to conveniently ignore this\n",
    "\n",
    "# An FFT returns a vector of complex values, in this case we are using the RFFT option to just return the real values \n",
    "\n",
    "def fspectra(data, dt = 4):\n",
    "    \"\"\"\n",
    "    Calculate the frequency spectra\n",
    "    \"\"\"\n",
    "    # Amplitude values\n",
    "    \n",
    "    # Get the absolute value of the Fourier coefficients\n",
    "    fc = np.abs(np.fft.rfft(data, axis = -1))\n",
    "               \n",
    "    # Take the mean to get the amplitude values of the spectra\n",
    "    a = np.mean(fc, axis = (0, 1))\n",
    "\n",
    "    # Get the frequency values corresponding to the coefficients\n",
    "    # We need the length of the window and the sample interval in seconds \n",
    "                \n",
    "    dts = dt / 1000\n",
    "    length = data.shape[-1]\n",
    "                \n",
    "    f = np.fft.rfftfreq(length, d = dts)\n",
    "               \n",
    "    return f, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amplitude spectra of the base and monitor from 1.5 - 2.5 seconds\n",
    "\n",
    "freq, ampb = fspectra(volve_ds['base'].sel(twt=slice(1500,2500)))\n",
    "_, ampm = fspectra(volve_ds['monitor'].sel(twt=slice(1500,2500)))\n",
    "\n",
    "# Plot the frequency spectra\n",
    "\n",
    "plt.figure(figsize = (11,6))\n",
    "\n",
    "plt.plot(freq, ampb, color='r')\n",
    "plt.plot(freq, ampm, color='b')\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(labels = ['Base','Monitor'])\n",
    "plt.title('Amplitude spectra')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace487e5",
   "metadata": {},
   "source": [
    "### Frequency filter\n",
    "There's a very small difference between the two vintages and possibly some noise above 40Hz, so let's apply a high cut filter <br>\n",
    "We're going to use the [signal processing options of SciPy](https://docs.scipy.org/doc/scipy/reference/signal.html)\n",
    "in a 2 step process; firstly to generate a filter and then to apply the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa4a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a lowpass/highcut filter\n",
    "\n",
    "# Firstly we'll define the Nyquist frequency\n",
    "\n",
    "fs = 1 / 0.004\n",
    "nyq = fs / 2\n",
    "\n",
    "# We're going to generate a simple butterworth filter, \n",
    "# The order (in this example we've used 7) determines the slope, larger is a steeper slope\n",
    "# The cutoff is the -3dB point and is given as a fraction of the nyquist frequency\n",
    "\n",
    "fcut = 40\n",
    "cutoff = fcut / nyq\n",
    "\n",
    "hc_filt = signal.butter(7, cutoff, btype='lowpass', output='sos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c389a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the response of the filter\n",
    "\n",
    "# The response is given angular freq (w), normalized to [0, pi]  and amplitude (h)\n",
    "\n",
    "w, h = signal.sosfreqz(hc_filt)\n",
    "\n",
    "# Convert to frequency\n",
    "# w_max / pi = nyq\n",
    "\n",
    "freq = nyq * w / np.pi\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.plot(freq, np.abs(h), 'b')        #plot the response\n",
    "plt.axvline(fcut, color='k', ls='--') #plot the high cut frequency as a dashed black line\n",
    "plt.plot(fcut, 1/np.sqrt(2), 'ko')    #plot the -3dB (1/sqrt(2)) point as a black circle\n",
    "\n",
    "plt.xlim(0, nyq)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.title('Filter response')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5380d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "\n",
    "volve_ds['base_hc40'] = (['il','xl','twt'], signal.sosfiltfilt(hc_filt, volve_ds['base'], axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea8b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amplitude spectra of the filtered base survey\n",
    "\n",
    "f, ampf = fspectra(volve_ds['base_hc40'].sel(twt=slice(1500,2500)))\n",
    "\n",
    "# Plot the frequency spectra\n",
    "\n",
    "plt.figure(figsize=(11,6))\n",
    "\n",
    "plt.plot(f, ampb, color='r')\n",
    "plt.plot(f, ampf, color='b')\n",
    "\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(labels = ['Base','Base after High Cut'])\n",
    "plt.title('Amplitude spectra')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321a6c8",
   "metadata": {},
   "source": [
    "A standard QC we want for any geophysical process is before, after and a difference plot.  We'll use the interactive plotting method we used to view the 4D difference but this time, rather than calculating the difference in advance, we'll do it on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75751c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot with before, after and difference\n",
    "# Set up a dictionary for our data selection panel \n",
    "\n",
    "seismic = {}\n",
    "seismic['before']     = volve_ds['base']\n",
    "seismic['filter']     = volve_ds['base_hc40']\n",
    "seismic['difference'] = volve_ds['base_hc40'] - volve_ds['base']\n",
    "\n",
    "# Define out plotting function\n",
    "\n",
    "def plot_compare(inl, volume):\n",
    "    da = seismic[volume].sel(il=inl)  \n",
    "    p = da.hvplot.image(clim=(-vm,vm), cmap='gray', flip_yaxis=True, invert=True) \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_sel = pn.widgets.Select(name='INL Selection', options=volve_ds.coords['il'].values.tolist())\n",
    "\n",
    "pn.interact(plot_compare, volume=seismic.keys(), inl=line_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db523d95",
   "metadata": {},
   "source": [
    "If we had more time we could obviously now filter the monitor survey and re-calculate the 4D difference and NRMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the cleanup function to remove the segy and sgz files if desired\n",
    "\n",
    "#cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb97cc",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "This tutorial aimed to give a geophysicist relatively new to Python a brief overview on how to read and view a SEGY dataset and run some simple processing and some of the packages that are available to them. <br>\n",
    "There are a number of additional resources with notebooks providing further examples, two useful locations are: \n",
    "\n",
    "- [SEG tutorials](https://github.com/seg/tutorials)\n",
    "- [Agile Github](https://github.com/agile-geoscience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95d0a4",
   "metadata": {},
   "source": [
    "## oneseismic Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b778de1",
   "metadata": {},
   "source": [
    "## Thanks\n",
    "\n",
    "We hope this tutorial has taught you some new tips and/or provided you with a basic introduction to help you start using Python to work with seismic data.\n",
    "\n",
    "Feel free to ask any follow up questions or start a conversation in our slack channel t21-thurs-practicalseismic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557c226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:t21_seis]",
   "language": "python",
   "name": "conda-env-t21_seis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
