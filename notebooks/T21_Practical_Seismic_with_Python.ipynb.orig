{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![swung logo](../images/swung_logo_vector.png)\n",
    "# Transform 21: Practical Seismic in Python #\n",
    "# Graeme Mackenzie & JÃ¸rgen Kvalsvik (Equinor) #\n",
    "\n",
    "## Introduction\n",
    "\n",
    "During this tutorial we are going to demonstrate how to read in a 4D base and monitor volumes using 2 different python packages, plot some lines and slices interactively then calculate some simple 4D attributes (4D difference and NRMS) and apply a frequency filter to the data.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### Python Packages\n",
    "\n",
    "In addition to the standard packages we will need\n",
    " - segyio\n",
    " - seismic-zfp\n",
    " - scipy\n",
    " - pyviz\n",
    " - holoviz\n",
    " \n",
    "If you are using Anaconda these can be installed using:\n",
    "\n",
    "    conda install segyio scipy \n",
    "    conda install -c pyviz holoviz\n",
    "    pip install seismic-zfp\n",
    " \n",
    "If you are using python for windows these will all need to be installed through pip install. However it is much easier to install holoviz using Anaconda (if you are using pip you will likely need to download and install the individual whl files)\n",
    "\n",
    "#### Data\n",
    "\n",
    "We are going to use 2 volumes from the Volve data which has been published by Equinor.  You can read about it [here](https://www.equinor.com/en/news/14jun2018-disclosing-volve-data.html) and download the data [here](https://data.equinor.com/dataset/Volve)\n",
    "\n",
    "We will use data from the 4D processing of the ST0202 and ST10010 surveys which are in the ST0202vsST10010_4D sub directory\n",
    "\n",
    "- ST0202ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534.segy\n",
    "- ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534.segy\n",
    "\n",
    "\n",
    "Instructions for downloading the data using Azure Storage Explorer are provided on the download link but we can also download the SEG-Ys using the azure cli and the code in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download the SEG-Ys from volve we use the azure cli\n",
    "import sys\n",
    "!{sys.executable} -m pip install azure-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# The shared access signature URI can be found on the Volve download page. The token is the stuff after ?, should be sv=<date>&sr=<sr>...\n",
    "sastoken = '<sas-token-here>'\n",
    "if not os.path.exists('../data/ST10010.segy'):\n",
    "    !{sys.executable} -m azure.cli storage blob download \\\n",
    "        --account-name datavillagesa \\\n",
    "        --container-name volve \\\n",
    "        --name \"Seismic/ST0202vsST10010_4D/Stacks/ST10010ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534.segy\" \\\n",
    "        --file \"../data/ST10010.segy\" \\\n",
    "        --sas-token \"{sastoken}\"\n",
    "\n",
    "if not os.path.exists('../data/ST0202.segy'):\n",
    "    !{sys.executable} -m azure.cli storage blob download \\\n",
    "        --account-name datavillagesa \\\n",
    "        --container-name volve \\\n",
    "        --name \"Seismic/ST0202vsST10010_4D/Stacks/ST0202ZDC12-PZ-PSDM-KIRCH-FULL-T.MIG_FIN.POST_STACK.3D.JS-017534.segy\" \\\n",
    "        --file \"../data/ST0202.segy\" \\\n",
    "        --sas-token \"{sastoken}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Run this to have the notebook clean up after itself. It is not run automatically.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.remove('../data/ST0202.segy')\n",
    "    os.remove('../data/ST10010.segy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports\n",
    "\n",
    "We'll start with importing some packages; <br>\n",
    "- NumPy (which we will use to hold the data once we have read it in)\n",
    "- Segyio (to read the data)\n",
    "- matlotlib for visualizing the data \n",
    "- time just so we can get some statistics on how long things take to run\n",
    "\n",
    "***Do we need pandas?  (possible if we are reading the headers)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "solar-cover",
=======
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import segyio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default plot size for matplotlib figures\n",
    "matplotlib.rcParams['figure.figsize'] = (11.75, 8.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Volve 2002 base survey using segyio\n",
    "\n",
    "We will read the base 2002 survey using segyio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "distant-simpson",
=======
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   "metadata": {},
   "outputs": [],
   "source": [
    "base_segy = '../data/ST0202.segy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "packed-transition",
=======
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   "metadata": {},
   "outputs": [],
   "source": [
    "with segyio.open(base_segy) as segyf:\n",
    "    n_traces = segyf.tracecount\n",
    "    sample_rate = segyio.tools.dt(segyf)\n",
    "    n_samples = segyf.samples.size\n",
    "    n_il = len(segyf.iline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlight problem is that the segy is not a perfect cube (number of inlines * number of xlines = number of traces).<br>\n",
    "There are a number of possible solutions to this. Here use the solution given in one of the segyio example notebooks\n",
    "https://github.com/equinor/segyio-notebooks/blob/master/notebooks/pylops/01_seismic_inversion.ipynb\n",
    "\n",
    "***Need to check/change the naming of the variables since we will be reading 2 volumes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "stupid-drain",
=======
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   "metadata": {},
   "outputs": [],
   "source": [
    "f = segyio.open(base_segy, ignore_geometry = True)\n",
    "ntraces    = len(f.trace)\n",
    "inlines    = []\n",
    "crosslines = []\n",
    "\n",
    "for i in range(ntraces):\n",
    "    headeri = f.header[i]\n",
    "    inlines.append(headeri[segyio.su.iline])\n",
    "    crosslines.append(headeri[segyio.su.xline])\n",
    "\n",
    "print(f'{ntraces} traces')\n",
    "print(f'first 10 inlines: {inlines[:10]}')\n",
    "print(f'first 10 crosslines: {crosslines[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'i' is only used to look up the the right header, but segyio can manage this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inlines = []\n",
    "crosslines = []\n",
    "for h in f.header:\n",
    "    inlines.append(h[segyio.su.iline])\n",
    "    crosslines.append(h[segyio.su.xline])\n",
    "        \n",
    "print(f'{ntraces} traces')\n",
    "print(f'first 10 inlines: {inlines[:10]}')\n",
    "print(f'first 10 crosslines: {crosslines[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have the inlines and crosslines. It's not a perfect cube, so time to verify that and figure out which in/crossline pairs that are missing. Let's check the EBCDIC header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segyio.tools.wrap(f.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(segyio.tools.wrap(f.text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.samples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "loving-encounter",
   "metadata": {},
=======
   "metadata": {
    "scrolled": true
   },
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   "outputs": [],
   "source": [
    "import itertools\n",
    "uniqil = set(inlines)\n",
    "uniqxl = set(crosslines)\n",
    "real = set(zip(inlines, crosslines))\n",
    "grid = set(itertools.product(uniqil, uniqxl))\n",
    "missing = grid - real\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "invalid-mixer",
=======
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the traces then reshape in numpy\n",
    "t0 = time.time()\n",
    "f = segyio.open(base_segy, ignore_geometry=True)\n",
    "traces = segyio.collect(f.trace)[:]\n",
    "ntraces, nt = traces.shape\n",
    "\n",
    "t = f.samples[:]\n",
    "il = f.attributes(segyio.TraceField.INLINE_3D)[:]\n",
    "xl = f.attributes(segyio.TraceField.CROSSLINE_3D)[:]\n",
    "\n",
    "## Define regular IL and XL axes\n",
    "\n",
    "il_unique = np.unique(il)\n",
    "xl_unique = np.unique(xl)\n",
    "\n",
    "il_min, il_max = min(il_unique), max(il_unique)\n",
    "xl_min, xl_max = min(xl_unique), max(xl_unique)\n",
    "\n",
    "# Get line increment\n",
    "dt = t[1] - t[0]\n",
    "dil = min(np.unique(np.diff(il_unique)))\n",
    "dxl = min(np.unique(np.diff(xl_unique)))\n",
    "\n",
    "# Create grid and get number of inlines & xlines\n",
    "ilines = np.arange(il_min, il_max + dil, dil)\n",
    "xlines = np.arange(xl_min, xl_max + dxl, dxl)\n",
    "nil, nxl = ilines.size, xlines.size\n",
    "\n",
    "ilgrid, xlgrid = np.meshgrid(np.arange(nil),\n",
    "                             np.arange(nxl),\n",
    "                             indexing='ij')\n",
    "\n",
    "# JOKVA PORT THIS\n",
    "# Look-up table\n",
    "traces_indeces = np.full((nil, nxl), np.nan)\n",
    "iils = (il - il_min) // dil\n",
    "ixls = (xl - xl_min) // dxl\n",
    "traces_indeces[iils, ixls] = np.arange(ntraces)\n",
    "traces_available = np.logical_not(np.isnan(traces_indeces))\n",
    "\n",
    "# Reorganize traces in regular grid\n",
    "d = np.zeros((nil, nxl, nt))\n",
    "d[ilgrid.ravel()[traces_available.ravel()],\n",
    "  xlgrid.ravel()[traces_available.ravel()]] = traces\n",
    "# Return the time (in seconds to do this)\n",
    "sgy_r_time = time.time() - t0\n",
    "print(f'segy file with {ntraces} traces ({nil} inlines, {nxl} xlines) indexed and read in {sgy_r_time} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the outputs <br>\n",
    "Data is in a 3D numpy array and our inline, crossline and twt are also in separate numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data\n",
    "As geophysicists one of the first things we want to do once we've loaded some data is to view it. We're going to use matplotlib to vizualize a single (central) inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
<<<<<<< HEAD
    "imgplot = plt.imshow(d[nil // 2,:,:], cmap='gray', aspect='auto', vmin=-5, vmax=5)\n",
    "plt.show()"
=======
    "central = len(ilines) // 2\n",
    "imgplot = plt.imshow(d[central,:,:], cmap='gray', aspect='auto', vmin=-5, vmax=5)"
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it's definitely seismic but not oriented how we would expect.  Also the axis labels aren't really meaningful to us. <br>\n",
    "When using any plotting packages in Python we need to consider the origin and how the data is read.  Matplotlib assumes the origin is in the top right corner (which is fine for a section but possibly not what we would want if plotting a time slice) but we need to transpose the array so it plots with the time on the y-axis <br>\n",
    "We also need to read the arrays containing the travel time and xline numbers and add these to the plot as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Include the extent from the relevant arrays containing the time and xline\n",
    "extent = [xlines[0], xlines[-1], t[-1], t[0]]\n",
    "# Plot the central inline in the usual orientation by transposing the data\n",
    "imgplot = plt.imshow(d[nil // 2,:,:].T, cmap='gray', aspect='auto', vmin=-5, vmax=5, extent=extent)\n",
    "# Add some labels\n",
    "plt.xlabel('Xline')\n",
    "plt.ylabel('TWT (ms)')\n",
    "plt.show()"
=======
    "extent  = [xlines[0], xlines[-1], t[-1], t[0]]\n",
    "imgplot = plt.imshow(d[central,:,:].T, cmap='gray', aspect='auto', vmin=-5, vmax=5, extent=extent)\n",
    "# GRAEME Sort out adding labels\n",
    "ax.set_xlabel('Xline')\n",
    "ax.set_ylabel('TWT (ms)')"
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that whilst matplotlib has the origin for the plot at the top right, the extent is coded as (left, right, bottom, top)\n",
    "(see [here](https://matplotlib.org/stable/tutorials/intermediate/imshow_extent.html) for more details on origins and extents in matplotlib.)\n",
    "\n",
    "We can also plot a slice, in this case we don't need transpose the data, but we have changed the origin to the bottom left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Slice\n",
    "extent = [xlines[0], xlines[-1], ilines[-1], ilines[0]]\n",
    "#imgplot = plt.imshow(d[:,:,575], cmap='gray', origin='lower', aspect='auto', vmin=-5, vmax=5)\n",
    "imgplot = plt.imshow(d[:,:,75], cmap='gray', origin='lower', aspect='auto', vmin=-5, vmax=5, extent=extent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive plotting\n",
    "\n",
    "Ideally we would like to be quickly scan through some lines, rather than having to edit the index number of the data and rerun the cell above multiple times. <br>\n",
    "\n",
    "We're going to use the advance plotting options of hvplot and panel to create an interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import panel as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "automotive-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default image size\n",
    "from holoviews import opts\n",
    "\n",
    "opts.defaults(\n",
    "    opts.Image(\n",
    "        width=800, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-birmingham",
=======
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a plotting function, the input will be an array containing the inlines\n",
    "\n",
    "def plot_inl(inl):\n",
    "    idx = inl - ilines[0]\n",
    "    da = xr.DataArray(d[idx,:,:].T)    \n",
    "    p = da.hvplot.image(clim=(-5, 5), cmap='gray', flip_yaxis=True) \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inl(ilines[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.interact(plot_inl, inl = ilines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Xarray\n",
    "\n",
    "As with our original plot using matplotlib the interactive plot above has no useful axis info. <br>\n",
    "In the *plot_inl* function we had to convert our numpy array into an DataArray in order to allow us to plot it with hvplot.  However, Xarray has a lot more useful functionality that makes it ideal for using with seismic data.  Xarray simplifies working with multi-dimensional data and allows dimension, coordinate and attribute labels to be added (segysak utilises it and [Tony's tutorial on Tuesday](https://www.youtube.com/watch?v=hjzTH14va4o) provided some more information on the format).<br>\n",
    "Xarray has two data structures\n",
    "- DataArray: for a single data variable\n",
    "- DataSet: a container for multiple Data Arrays that share the same coordinates\n",
    "\n",
    "The figure below [(Hoyer & Hamman, 2017)](https://openresearchsoftware.metajnl.com/articles/10.5334/jors.148/) illustrates the concept of a dataset containing climate data\n",
    "![xarray example](../images/xarray.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Data Array\n",
    "da = xr.DataArray(data = d,\n",
    "                  dims = ['il','xl','twt'],\n",
    "                  coords = {'il': ilines,\n",
    "                          'xl': xlines,\n",
    "                          'twt': t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our inline, crossline and travel time held as coordinates within inside the same array as the data itself. <br>  This allows us to select data either using the standard numpy numerical indexing and slicing or using xarray's .sel label based indexing, allowing us to select data based on the coordinate. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.sel(twt = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da[:,:,749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select a range we use slice\n",
    "da.sel(twt = slice(2000,3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've converted the 2002 seismic into a DataArray, but as we're going to be reading another vintage, it's useful to convert it into a DataSet which we can add the 2010 monitor survey into. <br> \n",
    "We'll also add some attribute information both to the data array and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some attribute information to the datarray\n",
    "da.attrs['Year'] = '2002'\n",
    "da.attrs['Type'] = 'Final PSDM time converted'\n",
    "# Create a dataset\n",
    "volve_ds = da.to_dataset(name = 'base')\n",
    "# Add some attribute information to the dataset\n",
    "volve_ds.attrs['Country'] = 'Norway'\n",
    "volve_ds.attrs['Field'] = 'Volve'\n",
    "# Take a look\n",
    "volve_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data from the xarray dataset is similar to a dictionary key and value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a line from the base survey\n",
    "volve_ds['base'].sel(il = 9963)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Volve 2010 monitor survey using seismic-zfp\n",
    "\n",
    "Now we've read and reviewed one vintage, let's read the second\n",
    "\n",
    "Another option to get around the irregular geometry would be to use segysak (ref Tony's tutorial).  Here we'll use a third option and convert the segy file into another format.  We will use [seismic-zfp](https://github.com/equinor/seismic-zfp) which converts the SEG-Y to a compressed format [Wade, 2020](https://www.earthdoc.org/content/papers/10.3997/2214-4609.202032080).  We won't go into detail on this format but examples can be found on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismic_zfp.conversion import SegyConverter, SgzReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion from segy to seismic-zfp format\n",
    "\n",
    "First step is to convert the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_segy = '../data/ST10010.segy'\n",
    "monitor_sgz = '../data/ST10010.sgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgz_t0 = time.time()\n",
    "with SegyConverter(monitor_segy) as converter:\n",
    "    # Create a \"standard\" SGZ file with 8:1 compression, using in-memory method\n",
    "    converter.run(monitor_sgz, bits_per_voxel=4)\n",
    "sgz_elapse = time.time() - sgz_t0\n",
    "print(f'Converted to sgz in {sgz_elapse} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read seismic-zfp\n",
    "Now it's converted we can read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cube from zgy\n",
    "t0 = time.time()\n",
    "with SgzReader(monitor_sgz) as reader:\n",
    "    n_traces_m = reader.tracecount\n",
    "    n_il_m = reader.n_ilines\n",
    "    n_xl_m = reader.n_xlines\n",
    "    num_samples_m = reader.n_samples\n",
    "    ilines_m = reader.ilines\n",
    "    xlines_m = reader.xlines\n",
    "    data_m = reader.read_volume()\n",
    "zgy_r_time = time.time() - t0\n",
    "print(f'sgz file with {n_traces_m} traces ({n_il_m} inlines, {n_xl_m} xlines) read in {zgy_r_time} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that reading the sgz file took much less time than segyio but we have the initial overhead of converting the file.  Whether you use segyio, seismic-zfp or segysak comes down a lot of things; how often will you read the file, are the headers critical, personal preference, is the compression important, ..... <br>\n",
    "\n",
    "Now we've read the monitor survey, let's quickly check they are both the same shape and have the same inline, crossline range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of arrays\n",
    "print(f'2002 base survey has shape {d.shape}, 2010 monitor has shape {data_m.shape}')\n",
    "# Line ranges\n",
    "print(f'Base survey; inline: {ilines.min()} - {ilines.max()}, crossline: {xlines.min()} - {xlines.max()}')\n",
    "print(f'Monitor survey; inline: {ilines_m.min()} - {ilines_m.max()}, crossline: {xlines_m.min()} - {xlines_m.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a slice of both volumes as a quick qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "fig, axs = plt.subplots(1,2, figsize=(16,6))\n",
    "extent = [xlines[0], xlines[-1], ilines[0], ilines[-1]]\n",
    "axs[0].imshow(d[:,:,75], cmap='gray', origin='lower', aspect='auto', vmin=-5, vmax=5, extent=extent)\n",
=======
    "base    =      d[:, :, 75]\n",
    "monitor = data_m[:, :, 75]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (11,8))\n",
    "\n",
    "extent = [xlines[0], xlines[-1], ilines[-1], ilines[0]]\n",
    "axs[0].imshow(base, cmap='gray', origin='lower', aspect='auto', vmin=-5, vmax=5, extent=extent)\n",
>>>>>>> 7329c17010badf6f926bb2fef49dc5931a341a04
    "axs[0].set_title('2002 Base, Time Slice')\n",
    "axs[1].imshow(monitor, cmap='gray', origin='lower', aspect='auto', vmin=-5, vmax=5, extent=extent)\n",
    "axs[1].set_title('2010 Monitor, Time Slice')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the monitor survey to the volve xarray dataset\n",
    "da2 = xr.DataArray(data_m)\n",
    "da2.attrs['Year'] = '2010'\n",
    "volve_ds['monitor'] = (['il','xl','twt'],da2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volve_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - 10 MIN BREAK ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4D Analysis\n",
    "Now we have the 2 volumes we can calculate some simple 4D attributes\n",
    "- Calculate the 4D difference \n",
    "- Calculate the NRMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference of two arrays is simply array2 minus array1\n",
    "# diff = volve_ds['monitor'] - volve_ds['base']\n",
    "# This will create a new array diff\n",
    "# When we read in the monitor survey we read it into a numpy array then added it to the xarray. \n",
    "# Let's add the difference directly to the volve_ds dataset\n",
    "\n",
    "volve_ds['difference'] = (['il','xl','twt'], volve_ds['monitor'] - volve_ds['base'])\n",
    "# Review the dataset\n",
    "volve_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot with before, after and difference\n",
    "# Set up a dictionary for our data selection panel \n",
    "seismic = {}\n",
    "seismic['Base'] = volve_ds['base']\n",
    "seismic['Monitor'] = volve_ds['monitor']\n",
    "seismic['4D Difference'] = volve_ds['monitor'] - volve_ds['base']\n",
    "\n",
    "# Define a plotting function\n",
    "\n",
    "def plot_line_compare(inl, volume):\n",
    "    da = xr.DataArray(seismic[volume].sel(il=inl))    \n",
    "    p = da.hvplot.image(clim=(-2,2), cmap='gray', flip_yaxis=True, invert=True) \n",
    "    return p\n",
    "\n",
    "def plot_slice_compare(t, volume):\n",
    "    da = xr.DataArray(seismic[volume].sel(twt=t))    \n",
    "    p = da.hvplot.image(clim=(-2,2), cmap='gray') \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.interact(plot_line_compare, inl=volve_ds.coords['il'].values, volume=seismic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.interact(plot_slice_compare, t=volve_ds.coords['twt'].values.astype(int), volume=seismic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NRMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty difficult to see where the genuine 4D signal is. A standard 4D QC is NRMS (Kragh and Christie, 2002) which is a measure of similarity between two traces (from 0 - 200%). \n",
    "\n",
    "$$\n",
    "NRMS = \\frac{200 x RMS(a_t - b_t)}{RMS(a_t) + RMS (b_t)}\n",
    "$$\n",
    "\n",
    "Ideally we want this to be as low as possible in a window above the reservoir where there should be no difference\n",
    "\n",
    "Kragh, E. and Christie, P. (2002) Seismic repeatability, normalized RMS, and predictability. The Leading Edge, 21, 640-647, http://dx.doi.org/10.1190/1.1497316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions we need to calculate NRMS\n",
    "# NRMS = 200 x [rms (b-m) / rms(b) + rms(m)]\n",
    "\n",
    "def rms (data):\n",
    "    rms = np.sqrt(np.mean(data**2, axis=2))\n",
    "    return rms\n",
    "\n",
    "def nrms (monitor, base):\n",
    "    rmsB = rms(base)\n",
    "    rmsM = rms(monitor)\n",
    "    diff = monitor - base\n",
    "    rms_diff = rms(diff)\n",
    "    nrms = 200 * (rms_diff / (rmsB + rmsM))\n",
    "    return nrms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the NRMS and add to the xarray\n",
    "volve_ds['nrms_1_2'] = (['il','xl'], nrms(volve_ds['monitor'].sel(twt=slice(1000,2000)), volve_ds['base'].sel(twt=slice(1000,2000))))\n",
    "volve_ds['nrms_2_3'] = (['il','xl'], nrms(volve_ds['monitor'].sel(twt=slice(2000,3000)), volve_ds['base'].sel(twt=slice(2000,3000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some statistics\n",
    "mean1 = volve_ds['nrms_1_2'].mean().values\n",
    "mean2 = volve_ds['nrms_2_3'].mean().values\n",
    "print(f'Mean NRMS 1000-2000ms: {mean1}')\n",
    "print(f'Mean NRMS 2000-3000ms: {mean2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot as 2 subplots\n",
    "# add histogram??\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (16,8))\n",
    "extent = [xlines[0], xlines[-1], ilines[0], ilines[-1]]\n",
    "im1=axs[0].imshow(volve_ds['nrms_1_2'], cmap='viridis_r', origin='lower', aspect='auto', vmin=5, vmax=35, extent=extent)\n",
    "axs[0].set_title('NRMS 1000-2000ms')\n",
    "fig.colorbar(im1, ax=axs[0], orientation='horizontal', pad=0.06)\n",
    "im2=axs[1].imshow(volve_ds['nrms_2_3'], cmap='viridis_r', origin='lower', aspect='auto', vmin=5, vmax=20, extent=extent)\n",
    "axs[1].set_title('NRMS 2000-3000ms')\n",
    "fig.colorbar(im2, ax=axs[1], orientation='horizontal', pad=0.06)\n",
    "fig.suptitle('NRMS', size='xx-large', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volve_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency spectra and filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency spectra\n",
    "\n",
    "The 4D QC's showed the data has a lot of 4D noise.  Let's take a look at the frequency spectra. <br>\n",
    "Both the Numpy and SciPy packages have the ability to run an fft, here we are using Numpy but we could equally  have used SciPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a very basic function to calculate frequency spectra\n",
    "# As we are using an fft we should really taper or window the data to prevent edge effects, \n",
    "# but for this demonstration we are going to conveniently ignore this\n",
    "\n",
    "def fspectra(data, dt = 4, dB = False):\n",
    "    S = np.mean(np.abs(np.fft.fft(data, axis=-1)), axis=(0, 1))\n",
    "    freq = np.fft.fftfreq(len(S), d=dt/1000)\n",
    "    f, amp = freq[:S.size//2], np.abs(S[:S.size//2])\n",
    "    if dB:\n",
    "        amax = np.max(amp)\n",
    "        amp = 20 * np.log10(amp/amax)\n",
    "    return f, amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dB amplitude spectra of the base and monitor from 1 - 2 seconds)\n",
    "\n",
    "f, ampb = fspectra(volve_ds['base'].sel(twt=slice(1000,2000)), dB=True)\n",
    "_, ampm = fspectra(volve_ds['monitor'].sel(twt=slice(1000,2000)), dB=True)\n",
    "\n",
    "# Plot the frequency spectra\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(f, ampb, color='r')\n",
    "plt.plot(f, ampm, color='b')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('dB Amplitude')\n",
    "plt.legend(labels = ['Base','Monitor'])\n",
    "plt.title('Amplitude spectra')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency filter\n",
    "There's a very small difference between the two vintages above 40Hz, so let's apply a high cut filter <br>\n",
    "We're going to use the [signal processing options of SciPy](https://docs.scipy.org/doc/scipy/reference/signal.html)\n",
    "in a 2 step process; firstly to generate a filter and then to apply the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a lowpass/highcut filter\n",
    "\n",
    "# Firstly we'll define the Nyquist frequency\n",
    "fs = 1 / 0.004\n",
    "nyq = fs / 2\n",
    "\n",
    "# We're going to generate a simple butterworth filter, \n",
    "# The order (in this example we've used 7) determines the slope, larger is a steeper slope\n",
    "# The cutoff is the -3dB point and is given as a fraction of the nyquist frequency\n",
    "\n",
    "cutoff = 40 / nyq\n",
    "hc_filt = signal.butter(7, cutoff, btype='lowpass', output='sos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the response of the filter\n",
    "\n",
    "w, h = signal.sosfreqz(hc_filt)\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "plt.plot(nyq*w/np.pi, np.abs(h), 'b')\n",
    "plt.plot(cutoff*nyq, 0.5*np.sqrt(2), 'ko')\n",
    "plt.axvline(cutoff*nyq, color='k')\n",
    "plt.xlim(0, nyq)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.title('Filter response')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "\n",
    "volve_ds['base_hc40'] = (['il','xl','twt'],signal.sosfiltfilt(hc_filt, volve_ds['base'], axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review our dataset\n",
    "volve_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amplitude spectra of the filtered base survey\n",
    "ff, ampf = fspectra(volve_ds['base_hc40'].sel(twt=slice(1000,2000)), dB=True)\n",
    "\n",
    "# Plot the frequency spectra\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(f, ampb, color='r')\n",
    "plt.plot(ff, ampf, color='b')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('dB Amplitude')\n",
    "plt.legend(labels = ['Base','Base after High Cut'])\n",
    "plt.title('Amplitude spectra')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard QC we want for any geophysical process is before, after and a difference plot.  We'll use the interactive plotting method we used to view the 4D difference but this time, rather than calculating the difference in advance, we'll do it on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot with before, after and difference\n",
    "# Set up a dictionary for our data selection panel \n",
    "seismic = {}\n",
    "seismic['before'] = volve_ds['base']\n",
    "seismic['filter'] = volve_ds['base_hc40']\n",
    "seismic['difference'] = volve_ds['base_hc40'] - volve_ds['base']\n",
    "\n",
    "# Define out plotting function\n",
    "\n",
    "def plot_flip(inl, volume):\n",
    "    da = xr.DataArray(seismic[volume].sel(il=inl))    \n",
    "    p = da.hvplot.image(clim=(-2,2), cmap='gray', flip_yaxis=True, invert=True) \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.interact(plot_line_compare, inl=volve_ds.coords['il'].values, volume=seismic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had more time we could obviously now filter the monitor survey and re-calculate the 4D difference and NRMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "This tutorial aimed to give a geophysicist relatively new to Python a brief overview on how to read and view a SEGY dataset and run some simple processing and some of the packages that are available to them. <br>\n",
    "There are a number of additional resources with notebooks providing further examples, two useful locations are: \n",
    "\n",
    "- [SEG tutorials](https://github.com/seg/tutorials)\n",
    "- [Agile Github](https://github.com/agile-geoscience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneSeismic Demonstration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
